{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "PA1N_FsyICqf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas.plotting import scatter_matrix\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import model_selection\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn import ensemble\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron"
      ],
      "metadata": {
        "id": "ha54lkHG3DFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptronClass(object):\n",
        "    \"\"\"Perceptron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "      Passes over the training dataset.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    errors_ : list\n",
        "      Number of misclassifications (updates) in each epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_samples, n_features]\n",
        "          Training vectors, where n_samples is the number of samples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_samples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        rgen = np.random.RandomState(self.random_state)\n",
        "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
        "        self.errors_ = []\n",
        "\n",
        "        for _ in range(self.n_iter):\n",
        "            errors = 0\n",
        "            for xi, target in zip(X, y):\n",
        "                update = self.eta * (target - self.predict(xi))\n",
        "                self.w_[1:] += update * xi\n",
        "                self.w_[0] += update\n",
        "                errors += int(update != 0.0)\n",
        "            self.errors_.append(errors)\n",
        "        return self\n",
        "\n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
      ],
      "metadata": {
        "id": "5qWrvixm25AM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaline SGD"
      ],
      "metadata": {
        "id": "eZpWSL3R3Hmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdalineSGD(object):\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "      Passes over the training dataset.\n",
        "    shuffle : bool (default: True)\n",
        "      Shuffles training data every epoch if True to prevent cycles.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    cost_ : list\n",
        "      Sum-of-squares cost function value averaged over all\n",
        "      training samples in each epoch.\n",
        "\n",
        "        \n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None, batch_size=20):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.w_initialized = False\n",
        "        self.shuffle = shuffle\n",
        "        self.random_state = random_state\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_samples, n_features]\n",
        "          Training vectors, where n_samples is the number of samples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_samples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        self._initialize_weights(X.shape[1])\n",
        "        self.cost_ = []\n",
        "        for i in range(self.n_iter):\n",
        "            if self.shuffle:\n",
        "                X, y = self._shuffle(X, y)\n",
        "            cost = []\n",
        "            for xi, target in zip(X, y):\n",
        "                cost.append(self._update_weights(xi, target))\n",
        "            avg_cost = sum(cost) / len(y)\n",
        "            self.cost_.append(avg_cost)\n",
        "        return self\n",
        "\n",
        "    def partial_fit(self, X, y):\n",
        "        self._initialize_weights(X.shape[1])\n",
        "        self.cost_ = []\n",
        "        for i in range(self.n_iter):\n",
        "            if self.shuffle:\n",
        "                X, y = self._shuffle(X, y)\n",
        "            y = y.reshape(-1, 1)\n",
        "            batches_X = [X[k:k+self.batch_size] for k in range(0, X.shape[0], self.batch_size)]\n",
        "            batches_y = [y[k:k+self.batch_size] for k in range(0, y.shape[0], self.batch_size)]\n",
        "            cost = []\n",
        "            for Xi, target in zip(batches_X, batches_y):\n",
        "                cost.append(self._update_weights(Xi, target))\n",
        "            avg_cost = sum(cost) / len(batches_y)\n",
        "            self.cost_.append(avg_cost)\n",
        "        return self\n",
        "\n",
        "\n",
        "    def _shuffle(self, X, y):\n",
        "        \"\"\"Shuffle training data\"\"\"\n",
        "        r = self.rgen.permutation(len(y))\n",
        "        return X[r], y[r]\n",
        "    \n",
        "    def _initialize_weights(self, m):\n",
        "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
        "        self.rgen = np.random.RandomState(self.random_state)\n",
        "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n",
        "        self.w_initialized = True\n",
        "        \n",
        "    def _update_weights(self, xi, target):\n",
        "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
        "        output = self.activation(self.net_input(xi))\n",
        "        error = (target - output)\n",
        "        self.w_[1:] += self.eta * xi.dot(error)\n",
        "        self.w_[0] += self.eta * error\n",
        "        cost = (0.5) * np.square(error)\n",
        "        return cost\n",
        "    \n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
      ],
      "metadata": {
        "id": "_smiJRkt3BM2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaline GD"
      ],
      "metadata": {
        "id": "zUJMpyUB3NZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdalineGD(object):\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "      Passes over the training dataset.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    cost_ : list\n",
        "      Sum-of-squares cost function value in each epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_samples, n_features]\n",
        "          Training vectors, where n_samples is the number of samples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_samples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        rgen = np.random.RandomState(self.random_state)\n",
        "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
        "        self.cost_ = []\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "        for i in range(self.n_iter):\n",
        "            net_input = self.net_input(X)\n",
        "            # Please note that the \"activation\" method has no effect\n",
        "            # in the code since it is simply an identity function. We\n",
        "            # could write `output = self.net_input(X)` directly instead.\n",
        "            # The purpose of the activation is more conceptual, i.e.,  \n",
        "            # in the case of logistic regression (as we will see later), \n",
        "            # we could change it to\n",
        "            # a sigmoid function to implement a logistic regression classifier.\n",
        "            output = self.activation(net_input)\n",
        "            errors = (y - output)\n",
        "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
        "            self.w_[0] += self.eta * errors.sum()\n",
        "            squared_errors = np.float128(errors)**2\n",
        "            cost = 0.5 * squared_errors.sum()\n",
        "            cost = cost/2.0\n",
        "            self.cost_.append(cost.astype(np.float64))\n",
        "        return self\n",
        "\n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
      ],
      "metadata": {
        "id": "A-2bEbwn3KWR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ],
      "metadata": {
        "id": "ZyPyaSGDIM0S"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Perceptron_obj = PerceptronClass()\n",
        "Perceptron_skt = Perceptron(tol=1e-3, random_state=0)\n",
        "\n",
        "Perceptron_skt.fit(X,y)\n",
        "Perceptron_obj.fit(X,y)\n",
        "\n",
        "y_pred_obj=Perceptron_obj.predict(X_test)\n",
        "y_pred_skt=Perceptron_skt.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Sklearn: \"+str(metrics.accuracy_score(y_test,y_pred_skt)))\n",
        "\n",
        "\n",
        "print(\"Class: \"+str(metrics.accuracy_score(y_test,y_pred_obj)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei8COXhU4m_8",
        "outputId": "5a1f2e86-257c-431b-d607-c8464661891d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sklearn: 0.9440559440559441\n",
            "Class: 0.6293706293706294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SGD_SKT = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n",
        "SGD_SKT.fit(X,y)\n",
        "\n",
        "AdalineSGD_obj = AdalineSGD()\n",
        "AdalineSGD_obj.fit(X,y)\n",
        "\n",
        "y_pred_obj = AdalineSGD_obj.predict(X_test)\n",
        "y_pred_skt = SGD_SKT.predict(X_test)\n",
        "\n",
        "print(\"Sktlearn: \"+str(metrics.accuracy_score(y_test,y_pred_skt)))\n",
        "print(\"AdalineSGD: \"+str(metrics.accuracy_score(y_test,y_pred_obj)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9tix-rG8Zbo",
        "outputId": "26d1ea85-f7a7-45db-90bb-fcef88f12e1d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sktlearn: 0.986013986013986\n",
            "AdalineSGD: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-cd576667d06e>:97: RuntimeWarning: overflow encountered in square\n",
            "  cost = (0.5) * np.square(error)\n",
            "<ipython-input-42-cd576667d06e>:95: RuntimeWarning: invalid value encountered in add\n",
            "  self.w_[1:] += self.eta * xi.dot(error)\n",
            "<ipython-input-42-cd576667d06e>:96: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  self.w_[0] += self.eta * error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deu um erro a na hora de utilizar a classe do AdalineSGD e não consegui identificar.\n"
      ],
      "metadata": {
        "id": "5oKpASC8AG_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gd_skt = LogisticRegression(random_state=0)\n",
        "gd_obj = AdalineGD()\n",
        "\n",
        "\n",
        "gd_skt.fit(X,y)\n",
        "gd_obj.fit(X,y)\n",
        "\n",
        "\n",
        "y_pred_obj = gd_obj.predict(X_test)\n",
        "y_pred_skt = gd_skt.predict(X_test)\n",
        "\n",
        "print(\"Sktlearn: \"+str(metrics.accuracy_score(y_test,y_pred_skt)))\n",
        "print(\"AdalineSGD: \"+str(metrics.accuracy_score(y_test,y_pred_obj)))"
      ],
      "metadata": {
        "id": "sOEEM3iQIZ0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53612dce-f248-4bcd-d3c8-c701655a7410"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sktlearn: 0.951048951048951\n",
            "AdalineSGD: 0.6293706293706294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}